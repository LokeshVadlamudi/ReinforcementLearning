{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SARSA_MountainCar.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNGFwnV7hiVJaWrQ3D7Sh93",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LokeshVadlamudi/ReinforcementLearning/blob/main/SARSA_MountainCar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqQwMSW47u0p"
      },
      "source": [
        "#lets import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8etWO58Pr9Yi"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "env_name = \"MountainCar-v0\"\n",
        "env = gym.make(env_name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-BttYCZAyuB"
      },
      "source": [
        "#setting the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dWlCyBC8itB"
      },
      "source": [
        "n_states = 40  # number of states\n",
        "episodes = 10 # number of episodes\n",
        "\n",
        "initial_lr = 1.0 # initial learning rate\n",
        "min_lr = 0.005 # minimum learning rate\n",
        "gamma = 0.99 # discount factor\n",
        "max_steps = 300\n",
        "epsilon = 0.05\n",
        "\n",
        "env = env.unwrapped\n",
        "env.seed(0)         #setting environment seed to reproduce same result\n",
        "np.random.seed(0)   #setting numpy random number generation seed to reproduce same random numbers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCe0zoVVA5bv"
      },
      "source": [
        "#function for discretization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH8PKVFH9Iip"
      },
      "source": [
        "def discretization(env, obs):\n",
        "    \n",
        "    env_low = env.observation_space.low\n",
        "    env_high = env.observation_space.high\n",
        "    \n",
        "    env_den = (env_high - env_low) / n_states\n",
        "    pos_den = env_den[0]\n",
        "    vel_den = env_den[1]\n",
        "    \n",
        "    pos_high = env_high[0]\n",
        "    pos_low = env_low[0]\n",
        "    vel_high = env_high[1]\n",
        "    vel_low = env_low[1]\n",
        "    \n",
        "    pos_scaled = int((obs[0] - pos_low)/pos_den)  #converts to an integer value\n",
        "    vel_scaled = int((obs[1] - vel_low)/vel_den)  #converts to an integer value\n",
        "    \n",
        "    return pos_scaled,vel_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoduWlJCBBi4"
      },
      "source": [
        "#SARSA algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8RY_l1X9MOd"
      },
      "source": [
        "#Q table\n",
        "#rows are states but here state is 2-D pos,vel\n",
        "#columns are actions\n",
        "#therefore, Q- table would be 3-D\n",
        "\n",
        "q_table = np.zeros((n_states,n_states,env.action_space.n))\n",
        "total_steps = 0\n",
        "for episode in range(episodes):\n",
        "   obs = env.reset()\n",
        "   total_reward = 0\n",
        "   # decreasing learning rate alpha over time\n",
        "   alpha = max(min_lr,initial_lr*(gamma**(episode//100)))\n",
        "   steps = 0\n",
        "\n",
        "   #action for the initial state using epsilon greedy\n",
        "   if np.random.uniform(low=0,high=1) < epsilon:\n",
        "        a = np.random.choice(env.action_space.n)\n",
        "   else:\n",
        "        pos,vel = discretization(env,obs)\n",
        "        a = np.argmax(q_table[pos][vel])\n",
        "  \n",
        "   while True:\n",
        "      env.render()\n",
        "      pos,vel = discretization(env,obs)\n",
        "    \n",
        "      obs,reward,terminate,_ = env.step(a) \n",
        "      total_reward += abs(obs[0]+0.5) \n",
        "      pos_,vel_ = discretization(env,obs)\n",
        "\n",
        "      #action for the next state using epsilon greedy\n",
        "      if np.random.uniform(low=0,high=1) < epsilon:\n",
        "          a_ = np.random.choice(env.action_space.n)\n",
        "      else:\n",
        "          a_ = np.argmax(q_table[pos_][vel_])\n",
        "\n",
        "      #q-table update\n",
        "      q_table[pos][vel][a] = (1-alpha)*q_table[pos][vel][a] + alpha*(reward+gamma*q_table[pos_][vel_][a_])\n",
        "      steps+=1\n",
        "      if terminate:\n",
        "          break\n",
        "      a = a_\n",
        "   print(\"Episode {} completed with total reward {} in {} steps\".format(episode+1,total_reward,steps)) \n",
        "while True: #to hold the render at the last step when Car passes the flag\n",
        "   env.render()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS_srtam9UeB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}